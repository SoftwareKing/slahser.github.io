![](https://o4dyfn0ef.qnssl.com/image/2016-11-15-kube7-logo.png?imageView2/2/h/200) 

[上次](http://www.slahser.com/2016/11/10/关于kubernetes-1.4.5-搭建/)我们搭建了一套 demo 环境. 

那么这次算是后日谈,解决一些新生的与遗留的问题. 

> 果然什么技术栈落地都不是能光速完成的,基础设施初见杀远比写代码麻烦的多. 

本期内容: 

- 技术名词实战版
- heapster 资源可视化
- 搭建完成后 dns 问题 
- k8s自动删镜像的问题 

- - - - -- 

## 技术名词 

运行` kubectl get --help `可以看到一大串抽象与定义,如下: 

我将初见可以理解的部分中文写在了后面 

其他的比如磁盘挂载, nginx ingress 等到更多探索再写出. 


* clusters (valid only for federation apiservers) - 集群
* componentstatuses (aka 'cs') - 组件健康度 
* configmaps (aka 'cm')
* daemonsets (aka 'ds')
* deployments (aka 'deploy') - 带有当前pod创建状态的rc,驱动rs进行pod创建管理,可以看到进度与版本
* events (aka 'ev') 事件记录,也就是系统内的log
* endpoints (aka 'ep') - ip:port 组成的端点
* horizontalpodautoscalers (aka 'hpa') - pod的横向自动扩容,依据是cpu/qps等等
* ingress (aka 'ing') - 将 NodePort 暴露的负载均衡口子
* jobs
* limitranges (aka 'limits')
* nodes (aka 'no') - 实体机器
* namespaces (aka 'ns') - 命名空间,用来隔离. 默认 default, 组件在 kube-system 中
* petsets (alpha feature, may be unstable) - 有状态的pod,比如mysql,比如es. 有着固定的hostname,volume与启动顺序
* pods (aka 'po') - 抽象
* persistentvolumes (aka 'pv') - 独立于pod外的数据存储,可以外接glusterfs,ceph层网络存储
* persistentvolumeclaims (aka 'pvc') - 用于pv资源的组件
* quota
* resourcequotas (aka 'quota')
* replicasets (aka 'rs') - 支持集合label选择的rc,通常与deploy一起使用
* replicationcontrollers (aka 'rc') - 期望情形的定义,定义pod的运行状态
* secrets
* serviceaccounts (aka 'sa')
* services (aka 'svc') - pod 的聚合

- - - - -- 

## 资源占用可视化 

![](https://o4dyfn0ef.qnssl.com/image/2016-11-15-Screen%20Shot%202016-11-15%20at%2016.40.42.png?imageView2/2/h/400) 

```shell
git clone https://github.com/kubernetes/heapster.git
cd heapster
# 不要偷懒只在 master 上提前 pull 镜像,因为 pod 可能被 scheduler 分配到其他 node 上. 
# 虽然貌似 k8s 会在节点间直接传输镜像的样子(猜测) 
docker pull kubernetes/heapster_grafana:v2.6.0
docker pull kubernetes/heapster:canary
docker pull kubernetes/heapster_influxdb:v0.6
# create 之前还是提前 pull 镜像&修改拉取策略吧
kubectl create -f deploy/kube-config/influxdb/
``` 

实际看到文件夹内含6个 yaml,deploy 与 svc 各三份. 

- heapster - agent 与 k8s add-on 的作用
- influxdb - db
- grafana - ui

只是想达到如图效果很简单了,只要部署一个 heapster-svc 上来就够了,但是 metrics 数据也可以被记录到 influxdb 中配合 grafana 进行展现. 

```shell
kubectl create -f heaspter-deployment.yaml
kubectl create -f heaspter-service.yaml
```

这样即可达到上图的效果. 

- - - - -- 

### minimal配置  

那么使用完全版的话将以上三对儿生成完毕 

```
# 查看 nodeport
kubectl describe svc monitoring-grafana --namespace=kube-system
```

1. 访问 masterip:port 进入 grafana 管理界面,左侧 admin - admin 登陆 
2. 剩下的什么新建 influxdb 的步骤都不需要了,默认自带一个`http://monitoring-influxdb:8086`
3. 这个数据源是否靠谱还不确定,因为不知道你是否已经是 dns 生效过,那么 test connection 进行测试. 
4. 实际上一步换成你的 influsdb-svc 的 ip 也 ok. 

![](https://o4dyfn0ef.qnssl.com/image/2016-11-15-Screen%20Shot%202016-11-15%20at%2017.29.54.png?imageView2/2/h/400) 

- - - - -- 

### influxdb 配置 

同时如果你对 influxdb 的数据结构感兴趣的话,这里有一个 ui 界面供使用. 

修改[这里](https://github.com/kubernetes/heapster/tree/master/deploy/kube-config/influxdb),修改`influxdb-service.yaml` 

解开以下部分的注释: 

```
spec:
  # InfluxDB has a UI that can be used to query, uncomment the `http` port and expose
  # to access this service.
  type: NodePort
  ports:
  - name: http
    port: 80
    targetPort: 8083
  - name: api
    port: 8086
    targetPort: 8086
  selector:
    k8s-app: influxdb
```

而后重新 create deploy 与 service. 

那么describe 一下 monitoring-influxdb 得到下图的 nodeport

![](https://o4dyfn0ef.qnssl.com/image/2016-11-15-Screen%20Shot%202016-11-15%20at%2020.10.52.png?imageView2/2/h/400) 

- 80 - 3xxxx对应 ui - masterip:3xxxx 用于访问 ui
- 8086 - 3xxxx 对应数据访问 - internal ip用于访问数据

![](https://o4dyfn0ef.qnssl.com/image/2016-11-15-Screen%20Shot%202016-11-15%20at%2020.11.03.png?imageView2/2/h/400) 

可以通过这个查询界面来看数据结构与数据是否正确落入. 

- - - - -- 

### 图表配置 

[这里](https://github.com/kubernetes/heapster/blob/master/docs/storage-schema.md)是 heaspter 的 metrics 列表. 

grafana dashboard 配置纷繁复杂,我这一期肯定不会在这上面耽误太长时间,应该只是浅尝辄止就是了. 

根据上面列表的具体示意以及 grafana 配置查询语句时的联想功能 

基本上简单的数据图/表应该也能顺藤摸瓜搞个出来. 

- - - - -- 

## dns 问题 

1. 检查`kubectl get svc --namespace=kube-system`中 kube-dns 的内网 ip
2. 检查 master 中/etc/systemd/system/kubelet.service.d/10-kubeadm.conf中`KUBE_DNS_ARGS`是否与之相同
3. 修改并重启 kubelet 

> 这个问题不一定肯定会出现,而且官方最近在计划把 dns 转到10.16.0.0/8网段..这个以后再说吧

- - - - -- 

## 自动删镜像的问题 

这个问题我遇到两次了,有事莫名其妙本机上的镜像会消失,但是都是k8s scheduler并没有指派到本机的任务的镜像.. 

暂时还没找到解.应该是跟下图中Events有关系.  

![](https://o4dyfn0ef.qnssl.com/image/2016-11-16-Screen%20Shot%202016-11-16%20at%2016.21.37.png?imageView2/2/h/300) 

找到了!,在Kubernetes权威指南的361页,有容器与镜像的GC相关设置. 

- - - - --  
